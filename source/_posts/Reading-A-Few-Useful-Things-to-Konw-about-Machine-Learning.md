---
title: Paper | 一些关于机器学习你需要知道的事
date: 2017-08-04 23:18:54
tags:
---


[原文链接](http://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf)

# 摘要
机器学习算法能够通过从数据中泛化来解决一些重要的问题。本文总结了**12**条经验，包括如何避开一些陷阱、需要注意的问题以及一些问题的回答。

<!--more-->

# 引言
现有的机器学习算法有很多不同的类型，本文集中介绍最为成熟、使用最为广泛的**classification**算法。但所讨论的问题在所有的机器学习算法中都会涉及到。

**classifier**是一个输入为一个离散或连续值的**feature value**的向量，输出是一个离散值，即类别。**learner**的输入是**training set**,输出是一个**classifier**。验证learner的正确性就是验证classifier是否能够正确预测测试样例的类别。

# learning = representation + evaluation + optimization
当你意识到你所面临的问题最好用机器学习的算法来解决的时候，你可能会在成百上千的机器学习算法中迷失，**应该选择哪一种算法？**这个问题的关键在于你要先意识到，一个机器学习的算法是由：表示、评估、优化，这三个部分组成的。

## Representation
一个classifier必须要表示成某种形式的语言，计算机才能处理。因此，为一个learner选择一种**合适的表示**，对选择一个**可学习的classifier**的集合很重要，这个集合称为learner的**假设空间（hypothesis space）**。也就是说，如果一个classifier不在这个假设空间中，说明这个classifier是不可学习的。这里引出了一个问题：如何表示输入，也就是**应该使用什么样的feature**。

## Evaluation
一个evaluation函数（也成为objective function 或 scoring function）被用来判别classifiers的好坏。learning算法所使用的evaluation函数和classifier所要优化的函数可能是不一样的。

## Optimization
优化技术是learner的效率关键所在，优化技术同样帮助通过evaluation函数达到最优解从而判断得到目标classifer。

表一列出了这三个部分一些常用的技术/算法：

<!-- ![3components.png](https://i.loli.net/2017/08/06/5986766344c44.png) -->
![表一](3components.png)
这三个components也不是随机组合都可以的。例如，离散的representation一般和combinatorial的优化组合，连续的和连续的optimization组合。但随着算法的发展，在一些learner中每一种可能的组合都可能出现。

# 一切都是generalization说了算
机器学习的基本目标就是能在训练数据外进行很好的泛化。机器学习的初学者普遍会犯的错误，是在训练集上进行测试，从而得到了算法成功的错觉，而当得到的classifier在其他的新的数据上进行测试的时候可能会得到比随机还要糟糕的结果。从数据中分出一些作为测试集诚然会使训练数据变少，但我们可以通过cross-validation的方法来进行改善。

在机器学习早期，将训练数据和测试数据分开的标准并没有被广泛遵循。部分原因是因为如果learner的超参数如果很少的话，在训练数据和测试数据上的误差不会很大。但在很多灵活的classifier中，或者有很多features的线性classifiers中，严格的区分是主流。

以泛化作为目标，使得不像其他大部分的优化算法，我们不直接对目标函数进行求解，而是用训练误差替代测试误差进行求解，这样的做法可能是危险的。具体的求解过程会在后面几节介绍。但从正面的角度来看，由于目标函数只是真实目标的一个近似，所以我们可能不需要完全地优化求解。实际上，一个简单贪心搜索得到的局部最优有可能比全局最优要好。

# 只有数据是不够的
泛化作为目标还会导致另一个结果：只有数据是不够的。举个例子，当从一百万个样例中来学习100个变量的布尔值时，虽然数据已经很多了，但仍然会有$ 2^{100} - 10^6 $种情况没有考虑到。当缺乏足够多信息的时候，每个learner都需要在数据之外知道一些先验知识或设想，这样才能够足够好地进行泛化。

幸运的是，我们在现实世界中所要学习的函数并不是一组从数学理论中的平均分布推导出的可能函数。实际上，一些通用的假设，如smoothness，limited dependences, limited complexity等等在实际求解中都能得到很好的结果，这也是机器学习能够成功的很大一部分原因。

相关的推论认为，选择representation的标准就是哪一种知识可以在这种表示下很好地表达。举个例子，当我们有很多关于在我们的定义域中如何创建更多样例的知识，instance-based 方法就是一个好的选择。如果我们有概率依赖的相关知识，graphical model是一个好的选择。如果我们知道每一种类别的先验条件（if...then...），rules可能是最好的选择。


# overfitting有很多可能
当knowledge和data不够充分时，就会出现overfitting的情况，这也是机器学习最棘手的地方。当你的classifier在训练数据上有100%准确率，而在测试数据上只有50%，就说明出现了overfitting的情况。

很多人都听说过overfitting，但实际上overfitting有很多种形式，有时候它表现的不是那么得明显。一种理解overfitting的方法就是将generalization error分解成bias和variance。bias指的是learner在同一种错误上的程度，variance指的是learner持续学习与真实信息无关的随机错误。图一通过在面板上投掷来表示这二者。

![图一](fig1.png)

一个linear learner有高bias，因为当两种类别的边界不是一个超平面的时候，linear learner 是无法推到出来的。而决策树没有这个问题，因为决策树可以表示任何的布尔函数，但是另一方面，决策树可能有很高的variance。因为由同一个现象生成的不同的训练数据训练得到的决策树往往会很不一样（而实际上他们应该要一样才对）。同样的情况还会发生在optimization方法的选择上：beam search比greedy search的bias小，但variance会更高，因为beam search尝试了很多种假设。因此，与第一直觉相反，一个更powerful的learner并不一定就会比一个less powerful的好。

图二就说明了这一点。尽管真实的classifier是一组规则，但当训练数据有1000个时，naive Bayes的结果却比rule learner更加精确。这是因为naive Bayes假设边界是线性的。这是机器学习中的普遍现象：**strong false assumptions can be better than weak true ones**,这是因为后者需要更多的数据来避免overfitting.

![图二](fig2.png)

cross-validation可以帮助避免overfitting，但也不是灵丹妙药。当需要选择的hyperparameters太多的时候，也会开始overfitting。

除了cross-validation，还有很多方法来减轻overfitting。最常见的就是在评估函数中加入**regularization term**. 这种方法能让learner在结构复杂的时候受到惩罚，从而使得learner的结构更加简单，也更不容易overfitting。 另一种方法是在加入新的结构之前进行统计验证（如chi-square），以此来决定加入这种结构是否会改变类别的分布。当数据集不足的时候，这些方法都部分有效。此外，对于声称能够“解决”overfitting的都应该表示怀疑。 当要避免overftting（varaiance）的时候，就很容易陷入underfitting（bias）。

对overfitting一个普遍的误解就是，认为overfitting是由噪声（例如错误的分类数据）引起的，但当没有噪声的时候也会导致overfitting。例如，我们训练一个布尔型classifier，而这个classifier的feature就是训练集中的“true”类型的样本，它的形式就是训练集数据中的每一项数据的布尔形式。于是这个classifier在训练数据上会完全准确，但是对于测试数据，就会漏掉一些没有出现在训练集中的“true”类型的样例，无论训练数据是否有噪声。

**multiple testing的问题与overfitting密切相关**。标准的统计测试认为只有一个假设被验证，但实际上，现代的学习算法可以轻易地测试几百万个假设，这使得一些看起来很重要的东西可能实际上并无特别。举个例子，同一支基金10年里都获利，这乍一看很让人印象深刻。然而，如果当你知道有1000支基金，每支基金在某一年里的活力概率是50%，这看起来就会觉得那支10年都获利的基金只是运气好而已。这个问题可以通过修改重要性测试来将假设的数量考虑在内，但这会导致**underfitting**。一个更好的办法是控制**falsely accepted non-null hypotheses**的比例，也就是所熟知的**false discovery rate**。

# 高维看起来更加直观
机器学习中除了overfitting之外的最大问题就是**维度灾难**。这个形式是Bellman在1961年定义的，他发现，很多算法在输入是低维数据时工作的很好，但输入是高维时就变得棘手了，但在机器学习中涉及到的问题更多。随着样本维度的增长，泛化的难度会指数级增长，这是因为固定大小的训练集在输入空间中的占比变小了。即使是在一个相对较弱的维度（如100维）以及万亿的训练数据集，后者也只是输入空间的一小部分（约$ 10^{-18} $）。这也是使得机器学习变得必要和困难的原因。

smilarity-based推导可以打破高维的问题。设想一个最邻近分类器，以Hamming 距离作为相似衡量，只有$ x_1, x_2 $两个feature，这是一个相对比较简单的问题。但如果有另外98个无关的feature，这些噪声就会削弱$ x_1, x_2 $的信号，使得最邻近算法和随机猜测差不多了。

尽管其他98个feature都是无关的，但最邻近算法依然会被干扰。这是因为在高维中，所有的样例都很相似。举个例子，落在规律网格外的样子，以及一个测试样例$ x_t $。 如果网格是d维的，那么距离$ x_t $最近的2d个样例，都与他距离相等。所以随着维度的增加，越来越多的样例都是$ x_t $的最近邻，此时最近邻的选择就相当于随机选择了。

这只是高维普遍问题中的一个实例：我们的直观感受来自于三维世界，而这不能直接应用于高维世界。大多数多元高斯分布不是聚集在均值处，而是一个渐远的“shell”形式围绕在均值处，一个高维“橘子”的体积大部分在“果皮”上，而不是“果肉”上。如果一些样例平均分布在高维超立方体上，在某些维度之外一些样例会更接近超立方体的面而不是最近邻样例。如果我们通过超立方体来近似超球面，在高维中，超立方体的体积几乎都在超球体外。这对于使用近似法的算法机器学习算法来说并不是一个好消息。

有些人认为使用更多的feature没什么不好，他们最多就是不提供任何有用的新信息而已，但实际上使用更多feature的增益会被维度灾难所破坏。

幸运的是，"bleassing of non-uniformity"可以部分抵消维度灾难的影像。大多数应用的样例并不是均匀分布在样例空间中的，而是几种在某一个低维的子空间中。例如k近邻算法在手写数字识别问题中能工作的很好，因为每一个像素都可以看作是一个维度，但数字图片的空间比所有可能的图片空间远远要小。学习算法可以隐式利用低微的有效空间，或者显示地降维。


# 理论证明并不像表面看起来那样
机器学习的论文充满了理论证明，醉普遍的一种类型是为了保证好的泛化所需要的样例数目的范围。induction和deduction传统上的比较：在deduction中你可以保证结论的正确，induction则世事难料。这也是几个世纪以来的争议。近几年最主要的发展就是意识到，实际上induction的结果我们是可以保证的，尤其是当我们更倾向于概率论上的保证。

最基本的争议其实十分简单。把预测的错误率大于 $\epsilon$ 的称为bad classifier。一个bad classifier连续正确预测n个随机独立的样例的概率小于$ (1-\epsilon)^n$。令b表示在算法的假设空间H中的bad classifier的数目，那么通过union bound可以知道，这些分类其中至少有一个classifier可以正确预测的概率是$ b(1-\epsilon)^n$。那么假设算法计算得到一个连续预测正确的classifier，而这个classifier是bad的概率会小于$ |H|(1-\epsilon)^n$，其中$b <= |H|$。当要想让这个概率下降到$\delta$，就需要

$ n>\frac{ln(\frac{\delta}{|H|})}{ln(1-\epsilon)} >= \frac{1}{\epsilon}(ln|H| + ln \frac{1}{\epsilon})  $

不幸的是，对这个类型的证明需要有所保留地看待，这是因为，在这个情况下求得的**边界**是很松的。上述的边界条件要求样例数目是对数型地增长。然而最有趣的假设空间往往被认为是指数级增长的。举个例子，在d个布尔变量的布尔函数的空间，如果有e
个可能的不同的样例，那么就有$2^e$个不同的可能函数，因为总共有$2^d$个可能的样例，那么可能函数的总个数就为$2^{2^d}$.尽管对于假设空间来说，这只是指数级的增长，但这个边界还是很松，因为union bound是很悲观的。举个例子，有100个布尔型feature，假设空间是一个10层的决策树，为了保证在上述公式的边界中有$\delta = \epsilon = 1%$，我们需要50多万的训练样例。但在实际上，一小部分的数据就可以满足一个精准的学习。

我们需要注意，一个像这样的bound意味着什么。举个例子，如果你的learner得到的假设空间和训练数据一致，那么这个假设空间可能可以泛化得很好。也就是说，给定一个足够大的训练集，你的learner很有可能得到一个泛化能力比较好的假设或者是一个预测准确的假设。这个bound并没有说明如何选择一个好的假设空间，他只告诉我们，如果假设空间包括了true classifier，那么这个learner输出bad classifier的概率就会随着数据集的增大而下降。如果我们缩小假设空间，增高bound,这个假设空间会包括true classifier的机会也会变小。

另一个理论证明的常见类型是**渐近线**：给定无穷的数据，learner保证输出正确的classifier。这个结果是可靠的，但利用渐近线证明来选择一个learner可能会是草率的。实际上我们很少会在渐近区。并且因为bias-variance的权衡，如果在learner A在拥有无穷数据的时候比learner B好，那么通常在有限数据的情况下，B会比A好。

在机器学习中理论证明的主要角色不是作为实际决策的标准，而是作为算法设计的理解和驱动的源头。理论和实践的这种密切相关性也正是这些年机器学习发展的主要原因之一。但要注意的是，learning是以中国复杂的现象，只是因为一个learner有一个理论上的合理性并且在实践中证明可行，这不意味着前者是后者的原因（理论的合理性并不代表着实践的可行性）。


# 特征工程是关键
机器学习成败的最重要的因素是使用feature。如果你使用了很多独立的与类别相关性很好的feature，那么学习是容易的。但如果分类是feature的一个很复杂的函数，那么你可能不那么容易进行学习。通常原始数据并不好学，但是你可以从原始数据中构建feature来。这是机器学习中最需要下功夫的地方，通常也是最有趣的地方，直觉、创造性和“黑魔法”和技术储备都一样重要。

初学者一开始可能会惊讶，在一个机器学习项目中花在学习训练上的时间并不是很多，但如果考虑收集数据、数据清洗、数据预处理以及特征设计身上所花费的巨大时间和实验，这一切就变得合理。机器学习不是一个建立数据然后学习的一次性的过程，而是一个运行算法、分析结果、然后修改数据和算法然后重复的迭代过程。learning往往是最快的部分，这是因为learning的部分研究相对比较成熟。特征工程是更有难度的部分，因为它是domain-specific的，而学习算法是可以大范围通用的。然而，这二者并没有什么明确的分界线，这也是另一个为什么最有效的学习算法往往是一些利用了很多先验知识的算法。

当然，机器学习的一大目标就是使得越来越多的特征工程过程变得自动化。今天的一种普遍做法是自动地生成大量的候选特正，然后通过这些feature的类的information gain来选择最好的feature。但记住，一些单独看可能不相关的特征联合起来的时候可能是相关的。例如，一个类是k个输入特正的XOR操作，每一个特征单独看都和类无关。（XOR是机器学习中经典的反例）另一方面，运行一个有大量特征值作为输入的算法会消耗很多时间，或者导致overfitting。所以，根本上来讲，特征工程方面的工作是无法替代的。


# 更多的数据比一个更聪明的算法来的更重要
假设你现在正在建立一组最好的特征集合，但现在的classsifier还不够准确，你现在能做什么？现在有两种选择：一个是设计更好的学习算法，一个是收集更多的数据（更多样例、更多原始特征值等）。机器学习的研究者大多数更关心前者，但从实用方面来讲，往往更多的数据就能更快成功。根据经验法则，一个拥有很多很多数据的普通算法，要比一个更加聪明但数据也更少的算法要好。（笔记机器学习就是让数据来“干活”）

这就引出了另一个问题：**可扩展性**。计算机学科中，两大资源局限就是：时间和内存。在机器学习中，有第三大局限：训练数据。这导致一个悖论：尽管理论上来说，数据越多表示可以学习更加复杂的分类器，但由于复杂的分类器需要很长的训练时间，所以实践中往往更简单的分类器更容易被采用。有部分研究在专注于更快地学习复杂的分类器。

有部分原因是，使用更聪明的算法实际收益比你想象的药效，因为对于第一次近似来说，他们都是一样的。实际上，propositional rules 和神经网络在编码上是一样的，并且他们的表示之间也有着相似的关系。所有的learner都是通过将相近的样例组成同一个类别来工作的，最主要的不同就是"相近"的定义。对于非均匀分布的数据来说，learner可以生成很不一样的边界同时使得每一个区域的类别都可以被正确预测。这同时也解释了为什么powerful learner不稳定的时候仍然可以很准确。图三在2D的角度阐述了这一点，越高维效果越显著。

![图三](fig3.png)


作为一个规律，一般都会尝试最简单的learner（先尝试naive Bayes然后尝试logisti regression, 先尝试k近邻算法然后尝试svm）。有很多更复杂更强大的算法，但它们通常都难以使用，因为要得到好的结果他们往往需要更多的技巧而且算法内部不透明。

学习算法可以分成两种：一种的表示是固定大小的（如线性分类器），而另一部分的特征是随着数据的增长而变化的（如决策树）。后者也被称为无参数学习算法，而且往往会被用来学习更多参数，而不是一个参数化的算法。fixed-size的学习算法可以利用很多的数据，variable-size算法从理论上来讲给定足够的数据可以学出任意的函数，但实际上受限于算法（例如贪心算法往往会陷于局部最优解）或者计算开销是做不到的。另外，由于维度灾难问题，没有什么现有的数据可以说是充足的。由于这些原因，一个clever的算法只要你肯投入尽可能多的数据和计算资源，都可以得到比较好的结果。但设计学习算法和学习分类器之间没有明确的分界线。反之，任何给定的知识都可以被编码在学习算法中，或者从数据中学习得到。所以机器学习的项目往往会将算法设计作为一个很重要的部分，并且工业实践需要了解其中的知识。


最后，最大的瓶颈不是数据或CPU周期，而是人类周期。在研究论文中，算法常常用准确性和计算开销来衡量，但人类开销（设计算法、理解输入输出）往往很难衡量，但更加重要。因此这使得能够产生便于理解的输出（例如rules set）的算法受到更多青睐。

# 学习多种模型，而不仅仅是一个
在机器学习早期，人们都有自己最喜欢的学习算法，并且有一系列的先验推论来证明其优越性。人们常常费工夫去尝试许多相关的变种，然后选择最好的一个。系统的实验比较发现，不同的应用之间的最好的learner是不一样的。现在的许多功夫豆花在了尝试各种算法的各种变种，然后只选择最好的一个。但研究人员渐渐发现，联合几个不一样的learner，效果可能比只选择最好的一个好，并且止血药一点点额外的开销。

创建这样的模型集合是有一套标准的。最简单的一种技术我们称作**bagging**，我们通过对训练数据重新采样来生成若干个随机的训练集，然后在每个训练集上学习一个分类器，最终的结果通过**voting**来结合在一起。这种方法可行的原因在于它大大减小了variance的同时bias只有一点点的上升。在**boosting**中，通过对训练数据加权的方式来训练不同的分类器（分类错误的训练样本的权重会更大）。在**stacking**中，每一个独立的分类器的输出变成higher-level的学习算法的输入，而这个学习算法用来计算如何最好地将这些输入结合起来。

还有许多其它的技术，在Netflix比赛中，人们发现冠军就是将超过100多种的学习算法协调组合在一起，得到了最优的结果。

模型整合不应该和**Bayesian model averaging(BMA)**混在一起。BMA是理论上最优的算法，在BMA中，新样例的预测结果是通过对假设空间中的所有分类器的预测结果进行取平均得来的，权重是通过每个分类器对训练数据的解释能力以及我们所赋予的优先级来决定的。尽管二者表面上非常相似，ensemble和BMA是非常不同的。ensemble改变了假设空间（例如，从一颗决策树到所有决策树的线性组合），并且形式很多样。BMA对原始空间中的假设按照某种固定的公式来进行加权。BMA的权重和bagging或者boosting的算法很不一样：后者到比较平滑，而前者比较歪曲。这是由于单个最高权重的分类器常常占主导地位。BMA会高效地等价地只选择这一个。这也使得model ensemble称为机器学习工具的一个重要部分，而BMA却少有人用。


# 简单并不意味着精确
奥卡姆剃刀原理同样适用于机器学习。给定两个有着同样训练误差的分类器，更简单的那个往往会有更低的测试误差。但实际上有很多的反例，“免费午餐”理论这个时候就不再适用。

在前一节中我们讲到一个反例：model ensembles. 尽管训练误差已经达到了零，当添加分类器时，boosted ensemble的泛化误差仍然会增加。另一个反例是svm，一个可以有效拥有无限参数而不会overfitting的算法。因此和直观感受不同，模型参数的数量和他overfitting的趋势无关。

基于更小的空间允许假设用更短的编码来表示的这样一个基础，一个与假设空间大小复杂度等价地可以用更复杂的一面可以替代。理论证明那一节所讲到的边界也能被看作是表示越短的假设泛化能力越好。通过对我们有一些优先级的空间中的假设赋值更短的编码可以进一步改善这一点。但把这看作精确度和简化之间的tradeoff的一个证明就变成了循环论证：我们通过偏好设计让假设变得简单，如果他们的准确度更高这是因为我们的偏好更加准确，而不是因为我们选择的表示中假设是“简单”的。

事实上，更加复杂的地方在于很少有learner会对整个假设空间穷尽搜索。一个有巨大搜索空间的learner只尝试了部分假设，比起一个从更小一点的空间搜索更多的假设的learner要比较不容易overfit。Pearl指出，假设空间的大小知识一个粗略的导向，它说明将训练误差和测试误差联系在一起的是：假设的选择过程/方式。

Domingos对机器学习中的奥卡姆剃刀原理的主要争议和证明进行了调研。结论是**更倾向于选择简单的假设，因为简单是一个learner自身的有点，不是因为假设与准确性之间的联系。


# representable并不代表learnable
本质上所有variable-size learner的表示都是与表示的定理相关：“每个函数都可以被表示，或者使用这种表示来近似趋近”。然而，实际上一个函数可以被表示不代表它可以被学习。举个例子，标准的决策树无法学习训练数据以后的样例。在连续空间中，就算是表示一个有固定primitives的简单函数都需要无穷数量的components。此外，如果假设空间的evaluation函数有很多的局部最优，就算函数被表示出来，learner也很难学到这个函数的一个真正的近似。给定有限数据、时间和内存，标准的learner可以只学习所有可能的函数的一个很小的子集，并且这些子集对于有不同表示的learner来说都是不同的。因此问题的关键在于**“它能够表示吗”**。而这个问题就要求尝试很多的learner了。

一些函数的表示以指数增长的方式变得紧凑。结果，他们可能就只需要指数级别少的数据来学习这些函数。需要的learner通过对简单基函数的线性组合来进行学习。举个例子，svm的形式就是将驯良样例上的kernel结合起来。这种方式下n位的等价性表示就需要$2^n$个基函数。但如果使用更多层的表示（即，在输入输出之间有更多的步骤），等价性就可以被编码进线性大小的分类器中。现在机器学习中就有一大前沿研究是去寻找能够学习这些更深层表示的方法。


# correlation并不意味着causation
尽管相关性不代表因果关系这个观点显而易见。但是我们之前所讨论的learner只能从数据中学习相关性，而他们的输出结果进场被看作与表示有因果原因的关系。这种看法是错误的吗？如果是，为什么人们还这么做？

通常，学习可预测模型的目标是利用他们作为行动指导。如果我们发现啤酒和尿布经常一起买，那么也许把它们放在一起可以增加销售量。但缺乏实验很男去证明这一点。机器学习常常应用于观察数据，在这里，与实验数据不同，可预测值往往是learner不可控的，而实验数据是可控的。一些机器学习算法可能能够从观察数据中提取因果信息，但他们的应用实际上更加严格。另一方面，相关关系是一种潜在因果关联的信号，我们可以将它用作将来更深一步的调研（例如尝试明白因果链可能是什么样的）的指导。

许多研究者相信，因果关系只是一种convenient fiction（假说，只存在在构想中，现实世界中只能表示或者建模）。举个例子，在物理规则中就没有因果关系的概念。因果关系是否存在都是一个更深层次的物理理论问题，而没有肯定的回答，但对于机器学习来说有两个很实用的观点。一，无论我们是否把这个现象和问题称为“causal”，我们都更倾向于预测我们action带来的影响，而不仅仅是观察变量之间的关系。第二，如果你可以获取实验数据（例如给浏览者不同版本的网址），那么无论如何都要进行这个实验。


# 总结
正如所有的学科，机器学习也有很多很难获得的“民间哲学”，但这些对于成功很重要。这篇文章总结了一些显著的点。本文作者写了另一本非技术的机器学习的介绍书[**The Master Algorithm**](http://www.cs.washington.edu/homes/pedrod/class)。另外[推荐](http://www.videolectures.net)